{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pytest-split Documentation : https://jerry-git.github.io/pytest-split Source Code : https://github.com/jerry-git/pytest-split PyPI : https://pypi.org/project/pytest-split/ Pytest plugin which splits the test suite to equally sized \"sub suites\" based on test execution time. Motivation Splitting the test suite is a prerequisite for parallelization (who does not want faster CI builds?). It's valuable to have sub suites which execution time is around the same. pytest-test-groups is great but it does not take into account the execution time of sub suites which can lead to notably unbalanced execution times between the sub suites. pytest-xdist is great but it's not suitable for all use cases. For example, some test suites may be fragile considering the order in which the tests are executed. This is of course a fundamental problem in the suite itself but sometimes it's not worth the effort to refactor, especially if the suite is huge (and smells a bit like legacy). Additionally, pytest-split may be a better fit in some use cases considering distributed execution. Installation pip install pytest-split Usage First we have to store test durations from a complete test suite run. This produces .test_durations file which should be stored in the repo in order to have it available during future test runs. The file path is configurable via --durations-path CLI option. pytest --store-durations Then we can have as many splits as we want: pytest --splits 3 --group 1 pytest --splits 3 --group 2 pytest --splits 3 --group 3 Time goes by, new tests are added and old ones are removed/renamed during development. No worries! pytest-split assumes average test execution time (calculated based on the stored information) for every test which does not have duration information stored. Thus, there's no need to store durations after changing the test suite. However, when there are major changes in the suite compared to what's stored in .test_durations, it's recommended to update the duration information with --store-durations to ensure that the splitting is in balance. The splitting algorithm can be controlled with the --splitting-algorithm CLI option and defaults to duration_based_chunks . For more information about the different algorithms and their tradeoffs, please see the section below. CLI commands slowest-tests Lists the slowest tests based on the information stored in the test durations file. See slowest-tests --help for more information. Interactions with other pytest plugins pytest-random-order : \u26a0\ufe0f The default settings of that plugin (setting only --random-order to activate it) are incompatible with pytest-split . Test selection in the groups happens after randomization, potentially causing some tests to be selected in several groups and others not at all. Instead, a global random seed needs to be computed before running the tests (for example using $RANDOM from the shell) and that single seed then needs to be used for all groups by setting the --random-order-seed option. nbval : pytest-split could, in principle, break up a single IPython Notebook into different test groups. This most likely causes broken up pieces to fail (for the very least, package import s are usually done at Cell 1, and so, any broken up piece that doesn't contain Cell 1 will certainly fail). To avoid this, after splitting step is done, test groups are reorganized based on a simple algorithm illustrated in the following cartoon: where the letters (A to E) refer to individual IPython Notebooks, and the numbers refer to the corresponding cell number. Splitting algorithms The plugin supports multiple algorithms to split tests into groups. Each algorithm makes different tradeoffs, but generally least_duration should give more balanced groups. Algorithm Maintains Absolute Order Maintains Relative Order Split Quality duration_based_chunks \u2705 \u2705 Good least_duration \u274c \u2705 Better Explanation of the terms in the table: * Absolute Order: whether each group contains all tests between first and last element in the same order as the original list of tests * Relative Order: whether each test in each group has the same relative order to its neighbours in the group as in the original list of tests The duration_based_chunks algorithm aims to find optimal boundaries for the list of tests and every test group contains all tests between the start and end bounary. The least_duration algorithm walks the list of tests and assigns each test to the group with the smallest current duration. Demo with GitHub Actions Development Clone this repository Requirements: Poetry Python 3.7+ Create a virtual environment and install the dependencies poetry install Activate the virtual environment poetry shell Testing pytest Documentation The documentation is automatically generated from the content of the docs directory and from the docstrings of the public signatures of the source code. The documentation is updated and published as a Github project page automatically as part each release. Releasing Trigger the Draft release workflow (press Run workflow ). This will update the changelog & version and create a GitHub release which is in Draft state. Find the draft release from the GitHub releases and publish it. When a release is published, it'll trigger release workflow which creates PyPI release and deploys updated documentation. Pre-commit Pre-commit hooks run all the auto-formatters (e.g. black , isort ), linters (e.g. mypy , flake8 ), and other quality checks to make sure the changeset is in good shape before a commit/push happens. You can install the hooks with (runs for each commit): pre-commit install Or if you want them to run only for each push: pre-commit install -t pre-push Or if you want e.g. want to run all checks manually for all files: pre-commit run --all-files This project was generated using the wolt-python-package-cookiecutter template.","title":"Introduction"},{"location":"#pytest-split","text":"Documentation : https://jerry-git.github.io/pytest-split Source Code : https://github.com/jerry-git/pytest-split PyPI : https://pypi.org/project/pytest-split/ Pytest plugin which splits the test suite to equally sized \"sub suites\" based on test execution time.","title":"pytest-split"},{"location":"#motivation","text":"Splitting the test suite is a prerequisite for parallelization (who does not want faster CI builds?). It's valuable to have sub suites which execution time is around the same. pytest-test-groups is great but it does not take into account the execution time of sub suites which can lead to notably unbalanced execution times between the sub suites. pytest-xdist is great but it's not suitable for all use cases. For example, some test suites may be fragile considering the order in which the tests are executed. This is of course a fundamental problem in the suite itself but sometimes it's not worth the effort to refactor, especially if the suite is huge (and smells a bit like legacy). Additionally, pytest-split may be a better fit in some use cases considering distributed execution.","title":"Motivation"},{"location":"#installation","text":"pip install pytest-split","title":"Installation"},{"location":"#usage","text":"First we have to store test durations from a complete test suite run. This produces .test_durations file which should be stored in the repo in order to have it available during future test runs. The file path is configurable via --durations-path CLI option. pytest --store-durations Then we can have as many splits as we want: pytest --splits 3 --group 1 pytest --splits 3 --group 2 pytest --splits 3 --group 3 Time goes by, new tests are added and old ones are removed/renamed during development. No worries! pytest-split assumes average test execution time (calculated based on the stored information) for every test which does not have duration information stored. Thus, there's no need to store durations after changing the test suite. However, when there are major changes in the suite compared to what's stored in .test_durations, it's recommended to update the duration information with --store-durations to ensure that the splitting is in balance. The splitting algorithm can be controlled with the --splitting-algorithm CLI option and defaults to duration_based_chunks . For more information about the different algorithms and their tradeoffs, please see the section below.","title":"Usage"},{"location":"#cli-commands","text":"","title":"CLI commands"},{"location":"#slowest-tests","text":"Lists the slowest tests based on the information stored in the test durations file. See slowest-tests --help for more information.","title":"slowest-tests"},{"location":"#interactions-with-other-pytest-plugins","text":"pytest-random-order : \u26a0\ufe0f The default settings of that plugin (setting only --random-order to activate it) are incompatible with pytest-split . Test selection in the groups happens after randomization, potentially causing some tests to be selected in several groups and others not at all. Instead, a global random seed needs to be computed before running the tests (for example using $RANDOM from the shell) and that single seed then needs to be used for all groups by setting the --random-order-seed option. nbval : pytest-split could, in principle, break up a single IPython Notebook into different test groups. This most likely causes broken up pieces to fail (for the very least, package import s are usually done at Cell 1, and so, any broken up piece that doesn't contain Cell 1 will certainly fail). To avoid this, after splitting step is done, test groups are reorganized based on a simple algorithm illustrated in the following cartoon: where the letters (A to E) refer to individual IPython Notebooks, and the numbers refer to the corresponding cell number.","title":"Interactions with other pytest plugins"},{"location":"#splitting-algorithms","text":"The plugin supports multiple algorithms to split tests into groups. Each algorithm makes different tradeoffs, but generally least_duration should give more balanced groups. Algorithm Maintains Absolute Order Maintains Relative Order Split Quality duration_based_chunks \u2705 \u2705 Good least_duration \u274c \u2705 Better Explanation of the terms in the table: * Absolute Order: whether each group contains all tests between first and last element in the same order as the original list of tests * Relative Order: whether each test in each group has the same relative order to its neighbours in the group as in the original list of tests The duration_based_chunks algorithm aims to find optimal boundaries for the list of tests and every test group contains all tests between the start and end bounary. The least_duration algorithm walks the list of tests and assigns each test to the group with the smallest current duration. Demo with GitHub Actions","title":"Splitting algorithms"},{"location":"#development","text":"Clone this repository Requirements: Poetry Python 3.7+ Create a virtual environment and install the dependencies poetry install Activate the virtual environment poetry shell","title":"Development"},{"location":"#testing","text":"pytest","title":"Testing"},{"location":"#documentation","text":"The documentation is automatically generated from the content of the docs directory and from the docstrings of the public signatures of the source code. The documentation is updated and published as a Github project page automatically as part each release.","title":"Documentation"},{"location":"#releasing","text":"Trigger the Draft release workflow (press Run workflow ). This will update the changelog & version and create a GitHub release which is in Draft state. Find the draft release from the GitHub releases and publish it. When a release is published, it'll trigger release workflow which creates PyPI release and deploys updated documentation.","title":"Releasing"},{"location":"#pre-commit","text":"Pre-commit hooks run all the auto-formatters (e.g. black , isort ), linters (e.g. mypy , flake8 ), and other quality checks to make sure the changeset is in good shape before a commit/push happens. You can install the hooks with (runs for each commit): pre-commit install Or if you want them to run only for each push: pre-commit install -t pre-push Or if you want e.g. want to run all checks manually for all files: pre-commit run --all-files This project was generated using the wolt-python-package-cookiecutter template.","title":"Pre-commit"},{"location":"api_docs/","text":"API documentation algorithms Algorithms ( Enum ) An enumeration. TestGroup ( tuple ) TestGroup(selected, deselected, duration) __getnewargs__ ( self ) special Return self as a plain tuple. Used by copy and pickle. Source code in pytest_split/algorithms.py def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self ) __new__ ( _cls , selected : List [ nodes . Item ], deselected : List [ nodes . Item ], duration : float ) special staticmethod Create new instance of TestGroup(selected, deselected, duration) __repr__ ( self ) special Return a nicely formatted representation string Source code in pytest_split/algorithms.py def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self duration_based_chunks ( splits : int , items : List [ nodes . Item ], durations : Dict [ str , float ]) -> List [ TestGroup ] Split tests into groups by runtime. Ensures tests are split into non-overlapping groups. The original list of test items is split into groups by finding boundary indices i_0, i_1, i_2 and creating group_1 = items[0:i_0], group_2 = items[i_0, i_1], group_3 = items[i_1, i_2], ... :param splits: How many groups we're splitting in. :param items: Test items passed down by Pytest. :param durations: Our cached test runtimes. Assumes contains timings only of relevant tests :return: List of TestGroup Source code in pytest_split/algorithms.py def duration_based_chunks ( splits : int , items : \"List[nodes.Item]\" , durations : \"Dict[str, float]\" ) -> \"List[TestGroup]\" : \"\"\" Split tests into groups by runtime. Ensures tests are split into non-overlapping groups. The original list of test items is split into groups by finding boundary indices i_0, i_1, i_2 and creating group_1 = items[0:i_0], group_2 = items[i_0, i_1], group_3 = items[i_1, i_2], ... :param splits: How many groups we're splitting in. :param items: Test items passed down by Pytest. :param durations: Our cached test runtimes. Assumes contains timings only of relevant tests :return: List of TestGroup \"\"\" items_with_durations = _get_items_with_durations ( items , durations ) time_per_group = sum ( map ( itemgetter ( 1 ), items_with_durations )) / splits selected : \"List[List[nodes.Item]]\" = [[] for i in range ( splits )] deselected : \"List[List[nodes.Item]]\" = [[] for i in range ( splits )] duration : \"List[float]\" = [ 0 for i in range ( splits )] group_idx = 0 for item , item_duration in items_with_durations : if duration [ group_idx ] >= time_per_group : group_idx += 1 selected [ group_idx ] . append ( item ) for i in range ( splits ): if i != group_idx : deselected [ i ] . append ( item ) duration [ group_idx ] += item_duration return [ TestGroup ( selected = selected [ i ], deselected = deselected [ i ], duration = duration [ i ]) for i in range ( splits ) ] least_duration ( splits : int , items : List [ nodes . Item ], durations : Dict [ str , float ]) -> List [ TestGroup ] Split tests into groups by runtime. It walks the test items, starting with the test with largest duration. It assigns the test with the largest runtime to the group with the smallest duration sum. The algorithm sorts the items by their duration. Since the sorting algorithm is stable, ties will be broken by maintaining the original order of items. It is therefore important that the order of items be identical on all nodes that use this plugin. Due to issue #25 this might not always be the case. :param splits: How many groups we're splitting in. :param items: Test items passed down by Pytest. :param durations: Our cached test runtimes. Assumes contains timings only of relevant tests :return: List of groups Source code in pytest_split/algorithms.py def least_duration ( splits : int , items : \"List[nodes.Item]\" , durations : \"Dict[str, float]\" ) -> \"List[TestGroup]\" : \"\"\" Split tests into groups by runtime. It walks the test items, starting with the test with largest duration. It assigns the test with the largest runtime to the group with the smallest duration sum. The algorithm sorts the items by their duration. Since the sorting algorithm is stable, ties will be broken by maintaining the original order of items. It is therefore important that the order of items be identical on all nodes that use this plugin. Due to issue #25 this might not always be the case. :param splits: How many groups we're splitting in. :param items: Test items passed down by Pytest. :param durations: Our cached test runtimes. Assumes contains timings only of relevant tests :return: List of groups \"\"\" items_with_durations = _get_items_with_durations ( items , durations ) # add index of item in list items_with_durations_indexed = [ ( * tup , i ) for i , tup in enumerate ( items_with_durations ) ] # sort in ascending order sorted_items_with_durations = sorted ( items_with_durations_indexed , key = lambda tup : tup [ 1 ], reverse = True ) selected : \"List[List[Tuple[nodes.Item, int]]]\" = [[] for i in range ( splits )] deselected : \"List[List[nodes.Item]]\" = [[] for i in range ( splits )] duration : \"List[float]\" = [ 0 for i in range ( splits )] # create a heap of the form (summed_durations, group_index) heap : \"List[Tuple[float, int]]\" = [( 0 , i ) for i in range ( splits )] heapq . heapify ( heap ) for item , item_duration , original_index in sorted_items_with_durations : # get group with smallest sum summed_durations , group_idx = heapq . heappop ( heap ) new_group_durations = summed_durations + item_duration # store assignment selected [ group_idx ] . append (( item , original_index )) duration [ group_idx ] = new_group_durations for i in range ( splits ): if i != group_idx : deselected [ i ] . append ( item ) # store new duration - in case of ties it sorts by the group_idx heapq . heappush ( heap , ( new_group_durations , group_idx )) groups = [] for i in range ( splits ): # sort the items by their original index to maintain relative ordering # we don't care about the order of deselected items s = [ item for item , original_index in sorted ( selected [ i ], key = lambda tup : tup [ 1 ]) ] group = TestGroup ( selected = s , deselected = deselected [ i ], duration = duration [ i ]) groups . append ( group ) return groups ipynb_compatibility ensure_ipynb_compatibility ( group : TestGroup , items : list ) -> None Ensures that group doesn't contain partial IPy notebook cells. pytest-split might, in principle, break up the cells of an IPython notebook into different test groups, in which case the tests most likely fail (for starters, libraries are imported in Cell 0, so all subsequent calls to the imported libraries in the following cells will raise NameError ). Source code in pytest_split/ipynb_compatibility.py def ensure_ipynb_compatibility ( group : \"TestGroup\" , items : list ) -> None : \"\"\" Ensures that group doesn't contain partial IPy notebook cells. ``pytest-split`` might, in principle, break up the cells of an IPython notebook into different test groups, in which case the tests most likely fail (for starters, libraries are imported in Cell 0, so all subsequent calls to the imported libraries in the following cells will raise ``NameError``). \"\"\" if not group . selected or not _is_ipy_notebook ( group . selected [ 0 ] . nodeid ): return item_node_ids = [ item . nodeid for item in items ] # Deal with broken up notebooks at the beginning of the test group first = group . selected [ 0 ] . nodeid siblings = _find_sibiling_ipynb_cells ( first , item_node_ids ) if first != siblings [ 0 ]: for item in list ( group . selected ): if item . nodeid in siblings : group . deselected . append ( item ) group . selected . remove ( item ) if not group . selected or not _is_ipy_notebook ( group . selected [ - 1 ] . nodeid ): return # Deal with broken up notebooks at the end of the test group last = group . selected [ - 1 ] . nodeid siblings = _find_sibiling_ipynb_cells ( last , item_node_ids ) if last != siblings [ - 1 ]: for item in list ( group . deselected ): if item . nodeid in siblings : group . deselected . remove ( item ) group . selected . append ( item ) plugin Base __init__ ( self , config : Config ) -> None special Load durations and set up a terminal writer. This logic is shared for both the split- and cache plugin. Source code in pytest_split/plugin.py def __init__ ( self , config : \"Config\" ) -> None : \"\"\" Load durations and set up a terminal writer. This logic is shared for both the split- and cache plugin. \"\"\" self . config = config self . writer = create_terminal_writer ( self . config ) try : with open ( config . option . durations_path ) as f : self . cached_durations = json . loads ( f . read ()) except FileNotFoundError : self . cached_durations = {} # This code provides backwards compatibility after we switched # from saving durations in a list-of-lists to a dict format # Remove this when bumping to v1 if isinstance ( self . cached_durations , list ): self . cached_durations = { test_name : duration for test_name , duration in self . cached_durations } PytestSplitCachePlugin ( Base ) The cache plugin writes durations to our durations file. pytest_sessionfinish ( self ) -> None Method is called by Pytest after the test-suite has run. https://github.com/pytest-dev/pytest/blob/main/src/_pytest/main.py#L308 Source code in pytest_split/plugin.py def pytest_sessionfinish ( self ) -> None : \"\"\" Method is called by Pytest after the test-suite has run. https://github.com/pytest-dev/pytest/blob/main/src/_pytest/main.py#L308 \"\"\" terminal_reporter = self . config . pluginmanager . get_plugin ( \"terminalreporter\" ) test_durations : \"Dict[str, float]\" = {} for test_reports in terminal_reporter . stats . values (): for test_report in test_reports : if isinstance ( test_report , TestReport ): # These ifs be removed after this is solved: # https://github.com/spulec/freezegun/issues/286 if test_report . duration < 0 : continue # pragma: no cover if ( test_report . when in ( \"teardown\" , \"setup\" ) and test_report . duration > STORE_DURATIONS_SETUP_AND_TEARDOWN_THRESHOLD ): # Ignore not legit teardown durations continue # pragma: no cover # Add test durations to map if test_report . nodeid not in test_durations : test_durations [ test_report . nodeid ] = 0 test_durations [ test_report . nodeid ] += test_report . duration if self . config . option . clean_durations : self . cached_durations = dict ( test_durations ) else : for k , v in test_durations . items (): self . cached_durations [ k ] = v with open ( self . config . option . durations_path , \"w\" ) as f : json . dump ( self . cached_durations , f , sort_keys = True , indent = 4 ) message = self . writer . markup ( \" \\n\\n [pytest-split] Stored test durations in {} \" . format ( self . config . option . durations_path ) ) self . writer . line ( message ) PytestSplitPlugin ( Base ) pytest_collection_modifyitems ( self , config : Config , items : List [ nodes . Item ]) -> None Collect and select the tests we want to run, and deselect the rest. Source code in pytest_split/plugin.py @hookimpl ( trylast = True ) def pytest_collection_modifyitems ( self , config : \"Config\" , items : \"List[nodes.Item]\" ) -> None : \"\"\" Collect and select the tests we want to run, and deselect the rest. \"\"\" splits : int = config . option . splits group_idx : int = config . option . group algo = algorithms . Algorithms [ config . option . splitting_algorithm ] . value groups = algo ( splits , items , self . cached_durations ) group = groups [ group_idx - 1 ] ensure_ipynb_compatibility ( group , items ) items [:] = group . selected config . hook . pytest_deselected ( items = group . deselected ) self . writer . line ( self . writer . markup ( f \" \\n\\n [pytest-split] Splitting tests with algorithm: { config . option . splitting_algorithm } \" ) ) self . writer . line ( self . writer . markup ( f \"[pytest-split] Running group { group_idx } / { splits } (estimated duration: { group . duration : .2f } s) \\n \" ) ) return None pytest_addoption ( parser : Parser ) -> None Declare pytest-split's options. Source code in pytest_split/plugin.py def pytest_addoption ( parser : \"Parser\" ) -> None : \"\"\" Declare pytest-split's options. \"\"\" group = parser . getgroup ( \"Split tests into groups which execution time is about the same. \" \"Run with --store-durations to store information about test execution times.\" ) group . addoption ( \"--store-durations\" , dest = \"store_durations\" , action = \"store_true\" , help = \"Store durations into '--durations-path'.\" , ) group . addoption ( \"--durations-path\" , dest = \"durations_path\" , help = ( \"Path to the file in which durations are (to be) stored, \" \"default is .test_durations in the current working directory\" ), default = os . path . join ( os . getcwd (), \".test_durations\" ), ) group . addoption ( \"--splits\" , dest = \"splits\" , type = int , help = \"The number of groups to split the tests into\" , ) group . addoption ( \"--group\" , dest = \"group\" , type = int , help = \"The group of tests that should be executed (first one is 1)\" , ) group . addoption ( \"--splitting-algorithm\" , dest = \"splitting_algorithm\" , type = str , help = f \"Algorithm used to split the tests. Choices: { algorithms . Algorithms . names () } \" , default = \"duration_based_chunks\" , choices = algorithms . Algorithms . names (), ) group . addoption ( \"--clean-durations\" , dest = \"clean_durations\" , action = \"store_true\" , help = ( \"Removes the test duration info for tests which are not present \" \"while running the suite with '--store-durations'.\" ), ) pytest_cmdline_main ( config : Config ) -> Optional [ Union [ int , ExitCode ]] Validate options. Source code in pytest_split/plugin.py @pytest . mark . tryfirst def pytest_cmdline_main ( config : \"Config\" ) -> \"Optional[Union[int, ExitCode]]\" : \"\"\" Validate options. \"\"\" group = config . getoption ( \"group\" ) splits = config . getoption ( \"splits\" ) if splits is None and group is None : return None if splits and group is None : raise pytest . UsageError ( \"argument `--group` is required\" ) if group and splits is None : raise pytest . UsageError ( \"argument `--splits` is required\" ) if splits < 1 : raise pytest . UsageError ( \"argument `--splits` must be >= 1\" ) if group < 1 or group > splits : raise pytest . UsageError ( f \"argument `--group` must be >= 1 and <= { splits } \" ) return None pytest_configure ( config : Config ) -> None Enable the plugins we need. Source code in pytest_split/plugin.py def pytest_configure ( config : \"Config\" ) -> None : \"\"\" Enable the plugins we need. \"\"\" if config . option . splits and config . option . group : config . pluginmanager . register ( PytestSplitPlugin ( config ), \"pytestsplitplugin\" ) if config . option . store_durations : config . pluginmanager . register ( PytestSplitCachePlugin ( config ), \"pytestsplitcacheplugin\" )","title":"API documentation"},{"location":"api_docs/#api-documentation","text":"","title":"API documentation"},{"location":"api_docs/#pytest_split.algorithms","text":"","title":"algorithms"},{"location":"api_docs/#pytest_split.algorithms.Algorithms","text":"An enumeration.","title":"Algorithms"},{"location":"api_docs/#pytest_split.algorithms.TestGroup","text":"TestGroup(selected, deselected, duration)","title":"TestGroup"},{"location":"api_docs/#pytest_split.algorithms.TestGroup.__getnewargs__","text":"Return self as a plain tuple. Used by copy and pickle. Source code in pytest_split/algorithms.py def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self )","title":"__getnewargs__()"},{"location":"api_docs/#pytest_split.algorithms.TestGroup.__new__","text":"Create new instance of TestGroup(selected, deselected, duration)","title":"__new__()"},{"location":"api_docs/#pytest_split.algorithms.TestGroup.__repr__","text":"Return a nicely formatted representation string Source code in pytest_split/algorithms.py def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self","title":"__repr__()"},{"location":"api_docs/#pytest_split.algorithms.duration_based_chunks","text":"Split tests into groups by runtime. Ensures tests are split into non-overlapping groups. The original list of test items is split into groups by finding boundary indices i_0, i_1, i_2 and creating group_1 = items[0:i_0], group_2 = items[i_0, i_1], group_3 = items[i_1, i_2], ... :param splits: How many groups we're splitting in. :param items: Test items passed down by Pytest. :param durations: Our cached test runtimes. Assumes contains timings only of relevant tests :return: List of TestGroup Source code in pytest_split/algorithms.py def duration_based_chunks ( splits : int , items : \"List[nodes.Item]\" , durations : \"Dict[str, float]\" ) -> \"List[TestGroup]\" : \"\"\" Split tests into groups by runtime. Ensures tests are split into non-overlapping groups. The original list of test items is split into groups by finding boundary indices i_0, i_1, i_2 and creating group_1 = items[0:i_0], group_2 = items[i_0, i_1], group_3 = items[i_1, i_2], ... :param splits: How many groups we're splitting in. :param items: Test items passed down by Pytest. :param durations: Our cached test runtimes. Assumes contains timings only of relevant tests :return: List of TestGroup \"\"\" items_with_durations = _get_items_with_durations ( items , durations ) time_per_group = sum ( map ( itemgetter ( 1 ), items_with_durations )) / splits selected : \"List[List[nodes.Item]]\" = [[] for i in range ( splits )] deselected : \"List[List[nodes.Item]]\" = [[] for i in range ( splits )] duration : \"List[float]\" = [ 0 for i in range ( splits )] group_idx = 0 for item , item_duration in items_with_durations : if duration [ group_idx ] >= time_per_group : group_idx += 1 selected [ group_idx ] . append ( item ) for i in range ( splits ): if i != group_idx : deselected [ i ] . append ( item ) duration [ group_idx ] += item_duration return [ TestGroup ( selected = selected [ i ], deselected = deselected [ i ], duration = duration [ i ]) for i in range ( splits ) ]","title":"duration_based_chunks()"},{"location":"api_docs/#pytest_split.algorithms.least_duration","text":"Split tests into groups by runtime. It walks the test items, starting with the test with largest duration. It assigns the test with the largest runtime to the group with the smallest duration sum. The algorithm sorts the items by their duration. Since the sorting algorithm is stable, ties will be broken by maintaining the original order of items. It is therefore important that the order of items be identical on all nodes that use this plugin. Due to issue #25 this might not always be the case. :param splits: How many groups we're splitting in. :param items: Test items passed down by Pytest. :param durations: Our cached test runtimes. Assumes contains timings only of relevant tests :return: List of groups Source code in pytest_split/algorithms.py def least_duration ( splits : int , items : \"List[nodes.Item]\" , durations : \"Dict[str, float]\" ) -> \"List[TestGroup]\" : \"\"\" Split tests into groups by runtime. It walks the test items, starting with the test with largest duration. It assigns the test with the largest runtime to the group with the smallest duration sum. The algorithm sorts the items by their duration. Since the sorting algorithm is stable, ties will be broken by maintaining the original order of items. It is therefore important that the order of items be identical on all nodes that use this plugin. Due to issue #25 this might not always be the case. :param splits: How many groups we're splitting in. :param items: Test items passed down by Pytest. :param durations: Our cached test runtimes. Assumes contains timings only of relevant tests :return: List of groups \"\"\" items_with_durations = _get_items_with_durations ( items , durations ) # add index of item in list items_with_durations_indexed = [ ( * tup , i ) for i , tup in enumerate ( items_with_durations ) ] # sort in ascending order sorted_items_with_durations = sorted ( items_with_durations_indexed , key = lambda tup : tup [ 1 ], reverse = True ) selected : \"List[List[Tuple[nodes.Item, int]]]\" = [[] for i in range ( splits )] deselected : \"List[List[nodes.Item]]\" = [[] for i in range ( splits )] duration : \"List[float]\" = [ 0 for i in range ( splits )] # create a heap of the form (summed_durations, group_index) heap : \"List[Tuple[float, int]]\" = [( 0 , i ) for i in range ( splits )] heapq . heapify ( heap ) for item , item_duration , original_index in sorted_items_with_durations : # get group with smallest sum summed_durations , group_idx = heapq . heappop ( heap ) new_group_durations = summed_durations + item_duration # store assignment selected [ group_idx ] . append (( item , original_index )) duration [ group_idx ] = new_group_durations for i in range ( splits ): if i != group_idx : deselected [ i ] . append ( item ) # store new duration - in case of ties it sorts by the group_idx heapq . heappush ( heap , ( new_group_durations , group_idx )) groups = [] for i in range ( splits ): # sort the items by their original index to maintain relative ordering # we don't care about the order of deselected items s = [ item for item , original_index in sorted ( selected [ i ], key = lambda tup : tup [ 1 ]) ] group = TestGroup ( selected = s , deselected = deselected [ i ], duration = duration [ i ]) groups . append ( group ) return groups","title":"least_duration()"},{"location":"api_docs/#pytest_split.ipynb_compatibility","text":"","title":"ipynb_compatibility"},{"location":"api_docs/#pytest_split.ipynb_compatibility.ensure_ipynb_compatibility","text":"Ensures that group doesn't contain partial IPy notebook cells. pytest-split might, in principle, break up the cells of an IPython notebook into different test groups, in which case the tests most likely fail (for starters, libraries are imported in Cell 0, so all subsequent calls to the imported libraries in the following cells will raise NameError ). Source code in pytest_split/ipynb_compatibility.py def ensure_ipynb_compatibility ( group : \"TestGroup\" , items : list ) -> None : \"\"\" Ensures that group doesn't contain partial IPy notebook cells. ``pytest-split`` might, in principle, break up the cells of an IPython notebook into different test groups, in which case the tests most likely fail (for starters, libraries are imported in Cell 0, so all subsequent calls to the imported libraries in the following cells will raise ``NameError``). \"\"\" if not group . selected or not _is_ipy_notebook ( group . selected [ 0 ] . nodeid ): return item_node_ids = [ item . nodeid for item in items ] # Deal with broken up notebooks at the beginning of the test group first = group . selected [ 0 ] . nodeid siblings = _find_sibiling_ipynb_cells ( first , item_node_ids ) if first != siblings [ 0 ]: for item in list ( group . selected ): if item . nodeid in siblings : group . deselected . append ( item ) group . selected . remove ( item ) if not group . selected or not _is_ipy_notebook ( group . selected [ - 1 ] . nodeid ): return # Deal with broken up notebooks at the end of the test group last = group . selected [ - 1 ] . nodeid siblings = _find_sibiling_ipynb_cells ( last , item_node_ids ) if last != siblings [ - 1 ]: for item in list ( group . deselected ): if item . nodeid in siblings : group . deselected . remove ( item ) group . selected . append ( item )","title":"ensure_ipynb_compatibility()"},{"location":"api_docs/#pytest_split.plugin","text":"","title":"plugin"},{"location":"api_docs/#pytest_split.plugin.Base","text":"","title":"Base"},{"location":"api_docs/#pytest_split.plugin.Base.__init__","text":"Load durations and set up a terminal writer. This logic is shared for both the split- and cache plugin. Source code in pytest_split/plugin.py def __init__ ( self , config : \"Config\" ) -> None : \"\"\" Load durations and set up a terminal writer. This logic is shared for both the split- and cache plugin. \"\"\" self . config = config self . writer = create_terminal_writer ( self . config ) try : with open ( config . option . durations_path ) as f : self . cached_durations = json . loads ( f . read ()) except FileNotFoundError : self . cached_durations = {} # This code provides backwards compatibility after we switched # from saving durations in a list-of-lists to a dict format # Remove this when bumping to v1 if isinstance ( self . cached_durations , list ): self . cached_durations = { test_name : duration for test_name , duration in self . cached_durations }","title":"__init__()"},{"location":"api_docs/#pytest_split.plugin.PytestSplitCachePlugin","text":"The cache plugin writes durations to our durations file.","title":"PytestSplitCachePlugin"},{"location":"api_docs/#pytest_split.plugin.PytestSplitCachePlugin.pytest_sessionfinish","text":"Method is called by Pytest after the test-suite has run. https://github.com/pytest-dev/pytest/blob/main/src/_pytest/main.py#L308 Source code in pytest_split/plugin.py def pytest_sessionfinish ( self ) -> None : \"\"\" Method is called by Pytest after the test-suite has run. https://github.com/pytest-dev/pytest/blob/main/src/_pytest/main.py#L308 \"\"\" terminal_reporter = self . config . pluginmanager . get_plugin ( \"terminalreporter\" ) test_durations : \"Dict[str, float]\" = {} for test_reports in terminal_reporter . stats . values (): for test_report in test_reports : if isinstance ( test_report , TestReport ): # These ifs be removed after this is solved: # https://github.com/spulec/freezegun/issues/286 if test_report . duration < 0 : continue # pragma: no cover if ( test_report . when in ( \"teardown\" , \"setup\" ) and test_report . duration > STORE_DURATIONS_SETUP_AND_TEARDOWN_THRESHOLD ): # Ignore not legit teardown durations continue # pragma: no cover # Add test durations to map if test_report . nodeid not in test_durations : test_durations [ test_report . nodeid ] = 0 test_durations [ test_report . nodeid ] += test_report . duration if self . config . option . clean_durations : self . cached_durations = dict ( test_durations ) else : for k , v in test_durations . items (): self . cached_durations [ k ] = v with open ( self . config . option . durations_path , \"w\" ) as f : json . dump ( self . cached_durations , f , sort_keys = True , indent = 4 ) message = self . writer . markup ( \" \\n\\n [pytest-split] Stored test durations in {} \" . format ( self . config . option . durations_path ) ) self . writer . line ( message )","title":"pytest_sessionfinish()"},{"location":"api_docs/#pytest_split.plugin.PytestSplitPlugin","text":"","title":"PytestSplitPlugin"},{"location":"api_docs/#pytest_split.plugin.PytestSplitPlugin.pytest_collection_modifyitems","text":"Collect and select the tests we want to run, and deselect the rest. Source code in pytest_split/plugin.py @hookimpl ( trylast = True ) def pytest_collection_modifyitems ( self , config : \"Config\" , items : \"List[nodes.Item]\" ) -> None : \"\"\" Collect and select the tests we want to run, and deselect the rest. \"\"\" splits : int = config . option . splits group_idx : int = config . option . group algo = algorithms . Algorithms [ config . option . splitting_algorithm ] . value groups = algo ( splits , items , self . cached_durations ) group = groups [ group_idx - 1 ] ensure_ipynb_compatibility ( group , items ) items [:] = group . selected config . hook . pytest_deselected ( items = group . deselected ) self . writer . line ( self . writer . markup ( f \" \\n\\n [pytest-split] Splitting tests with algorithm: { config . option . splitting_algorithm } \" ) ) self . writer . line ( self . writer . markup ( f \"[pytest-split] Running group { group_idx } / { splits } (estimated duration: { group . duration : .2f } s) \\n \" ) ) return None","title":"pytest_collection_modifyitems()"},{"location":"api_docs/#pytest_split.plugin.pytest_addoption","text":"Declare pytest-split's options. Source code in pytest_split/plugin.py def pytest_addoption ( parser : \"Parser\" ) -> None : \"\"\" Declare pytest-split's options. \"\"\" group = parser . getgroup ( \"Split tests into groups which execution time is about the same. \" \"Run with --store-durations to store information about test execution times.\" ) group . addoption ( \"--store-durations\" , dest = \"store_durations\" , action = \"store_true\" , help = \"Store durations into '--durations-path'.\" , ) group . addoption ( \"--durations-path\" , dest = \"durations_path\" , help = ( \"Path to the file in which durations are (to be) stored, \" \"default is .test_durations in the current working directory\" ), default = os . path . join ( os . getcwd (), \".test_durations\" ), ) group . addoption ( \"--splits\" , dest = \"splits\" , type = int , help = \"The number of groups to split the tests into\" , ) group . addoption ( \"--group\" , dest = \"group\" , type = int , help = \"The group of tests that should be executed (first one is 1)\" , ) group . addoption ( \"--splitting-algorithm\" , dest = \"splitting_algorithm\" , type = str , help = f \"Algorithm used to split the tests. Choices: { algorithms . Algorithms . names () } \" , default = \"duration_based_chunks\" , choices = algorithms . Algorithms . names (), ) group . addoption ( \"--clean-durations\" , dest = \"clean_durations\" , action = \"store_true\" , help = ( \"Removes the test duration info for tests which are not present \" \"while running the suite with '--store-durations'.\" ), )","title":"pytest_addoption()"},{"location":"api_docs/#pytest_split.plugin.pytest_cmdline_main","text":"Validate options. Source code in pytest_split/plugin.py @pytest . mark . tryfirst def pytest_cmdline_main ( config : \"Config\" ) -> \"Optional[Union[int, ExitCode]]\" : \"\"\" Validate options. \"\"\" group = config . getoption ( \"group\" ) splits = config . getoption ( \"splits\" ) if splits is None and group is None : return None if splits and group is None : raise pytest . UsageError ( \"argument `--group` is required\" ) if group and splits is None : raise pytest . UsageError ( \"argument `--splits` is required\" ) if splits < 1 : raise pytest . UsageError ( \"argument `--splits` must be >= 1\" ) if group < 1 or group > splits : raise pytest . UsageError ( f \"argument `--group` must be >= 1 and <= { splits } \" ) return None","title":"pytest_cmdline_main()"},{"location":"api_docs/#pytest_split.plugin.pytest_configure","text":"Enable the plugins we need. Source code in pytest_split/plugin.py def pytest_configure ( config : \"Config\" ) -> None : \"\"\" Enable the plugins we need. \"\"\" if config . option . splits and config . option . group : config . pluginmanager . register ( PytestSplitPlugin ( config ), \"pytestsplitplugin\" ) if config . option . store_durations : config . pluginmanager . register ( PytestSplitCachePlugin ( config ), \"pytestsplitcacheplugin\" )","title":"pytest_configure()"},{"location":"changelog/","text":"Changelog All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . Unreleased 0.6.0 - 2022-01-10 Added PR template Test against 3.10 Compatibility with IPython Notebooks 0.5.0 - 2021-11-09 Added Wolt cookiecutter + cruft setup, see https://github.com/jerry-git/pytest-split/pull/33 0.4.0 - 2021-11-09 Changed Durations file content in prettier format, see https://github.com/jerry-git/pytest-split/pull/31","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"Unreleased"},{"location":"changelog/#060-2022-01-10","text":"","title":"0.6.0 - 2022-01-10"},{"location":"changelog/#added","text":"PR template Test against 3.10 Compatibility with IPython Notebooks","title":"Added"},{"location":"changelog/#050-2021-11-09","text":"","title":"0.5.0 - 2021-11-09"},{"location":"changelog/#added_1","text":"Wolt cookiecutter + cruft setup, see https://github.com/jerry-git/pytest-split/pull/33","title":"Added"},{"location":"changelog/#040-2021-11-09","text":"","title":"0.4.0 - 2021-11-09"},{"location":"changelog/#changed","text":"Durations file content in prettier format, see https://github.com/jerry-git/pytest-split/pull/31","title":"Changed"}]}